[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre Mim",
    "section": "",
    "text": "Me chamo Lemuel e nos últimos 10 anos venho me dedicando à programação e à análise de dados. Sou formado em economia, com mestrado em economia e doutorado na área de finanças, todos pela UFRN. Atualmente, trabalho como analista de dados na área de finanças e sou apaixonado por programação e análise de dados. Ultimamente, dediquei uma boa parte do meu tempo estudando e trabalhando com Python e R na área de fundos de investimentos. Trabalho em um escritório de investimentos e sou responsável por toda a parte de análise de dados e programação, onde desenvolvo relatórios e dashboards de acompanhamento dos resultados da empresa.\nNa área acadêmica, dediquei-me bastante ao aprendizado de programação, sempre voltado para a utilização de ferramentas de pesquisa quantitativa. Já executei modelos que vão desde simples regressões até modelos de volatilidade com GARCH e DCC-GARCH, além de modelos de séries temporais, que variam desde os modelos clássicos de séries temporais (ARIMA’s) até modelos multivariados como VAR e VEC. Atualmente, meu foco está na pesquisa do comportamento dos fundos de investimento, onde analisei o comportamento do investidor frente às mudanças das variáveis macroeconômicas e o testei para análise da existência de gerenciamento de resultados por parte dos gestores, em relação à captura da taxa de performance. Recentemente, utilizei modelos de dados em painel e testes não paramétricos para diferença de médias.\nDurante o doutorado, comecei a trabalhar em um escritório de assessoria de investimentos, onde participei de todas as áreas, desde a mesa de operações de renda variável até a área de análise de dados e BI da empresa. Um ponto que merece destaque é que, além da área de análise de dados, desenvolvi diversos robôs para automação de rotinas administrativas e operacionais. Um dos grandes projetos que liderei na empresa foi a criação de um sistema de cálculo de comissões, o que representa um desafio significativo para empresas de assessorias. Esse sistema foi projetado com a personalização de diversas regras de negócio, proporcionando uma elevada flexibilidade e confiança nos processos de comissão. Anteriormente, todo o processo era realizado através de planilhas de Excel, o que era bastante trabalhoso e demorado. A implementação desse sistema nos permitiu economizar várias horas de trabalho.\nAlém disso, nosso outro grande projeto foi montar um dashboard que permite o acompanhamento diário da produção da mesa de operações e dos assessores. Esse dashboard possibilita a integração de diversos relatórios em bases de dados unificadas. Ele é desenvolvido utilizando as tecnologias Node/React e conta com uma API no backend em R/Python. Tenho como objetivo para os próximos meses integrar novas ferramentas de IA, como os modelos LLM, para fornecer uma interface mais interativa aos usuários, além dos dados apresentados via dashboards.\nCom base em toda essa experiência e aprendizado, decidi criar este blog para compartilhar conhecimento e insights. Aqui, vamos nos concentrar na criação de dashboards em R Shiny e Dash Plotly, na análise de dados da indústria de fundos e na modelagem de métricas de acompanhamento, especialmente relacionadas à indústria de fundos de investimento.\nSeja bem-vindo ao meu blog e espero que as informações compartilhadas aqui possam ser úteis para você, assim como têm sido para mim ao longo desses anos."
  },
  {
    "objectID": "curriculo.html",
    "href": "curriculo.html",
    "title": "Mutual Blog",
    "section": "",
    "text": "Olá, eu sou o Lemuel! Atualmente, exerço a função de líder operacional (Head Operacional) em um escritório de assessoria de investimentos. Na prática, isso significa que sou responsável por uma equipe de duas pessoas no backoffice e encarregado de gerenciar os dados, desde o ETL até o processo de transformar números em relatórios nos mais diversos formatos. Se precisar de alguém para dar um toque de criatividade nos dados e se tornar seu faxineiro de dados (data janitor), estou aqui!\n\n\nPortuguese: Nativo  English: Proficiente\n\n\n\n R / tidyverse / Shiny / Plumber / RMarkdown / Quarto   GitHub   Python / Selenium / Pandas / Streamlit   AWS \n\n\n\n-Ciências Econômicas  Local: UFRN 2010 - 2015  -Mestrado Economia  Local: UFRN 2016 - 2017  -Doutorado Finanças  Local: UFRN 2018 - 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n\n\n\nAxios Investimentos\n\n\nFunção\nLocal\nInicio\nFim\nDescrição\n\n\n\n\nOperador de Renda variável\nNatal, RN\njul/19\nmar/20\nAtuava no processo de automatização da mesa de renda variável, o qual envolvia a geração de ordens para diversos clientes com o objetivo de aderir às trocas de ativos sugeridos nas carteiras recomendadas pelas researchs que utilizamos. Esse processo incluiu a criação de um dashboard que gerenciava todas as etapas, desde o cadastro dos clientes e carteiras até a geração de baskets de ordens, as quais eram utilizadas para alimentar as plataformas de negociação.\n\n\n\n\n\n\n\n\n7RD Investimentos\n\n\nFunção\nLocal\nInicio\nFim\nDescrição\n\n\n\n\nBI\nNatal, RN\njul/20\nabr/22\nDesenvolvi ferramentas de BI na empresa, proporcionando dados para gestores e assessores por meio de dashboards. Criei um sistema eficiente para calcular comissões e desenvolvi robôs automatizando rotinas administrativas.\n\n\nOperador de Renda variável\nNatal, RN\nnov/21\njan/23\nAtuava como operador na mesa de operações, auxiliando os clientes no processo de envio de ordens de renda variável. Além disso, desenvolvi um dashboard de controle de produção da mesa, que indicava diariamente a produção em nível de operador, assessor e cliente.\n\n\nHead Operacional\nNatal, RN\nmar/22\nAtual\nAtuo como líder operacional, gerenciando os processos de análise de dados do escritório e as operações operacionais. Aqui, lidero o time de backoffice, composto principalmente por assessores em treinamento, e resolvo problemas mais complexos quando necessário. Além dos processos operacionais habituais, também estou envolvido na automação das rotinas financeiras, como o cálculo de impostos e comissões.\n\n\n\n\n\n\n\n\n\n\nCurrículo feito com o Quarto. Última Atualização em abr 27, 2024.  Código Disponível em  GitHub.  License: CC BY-SA 2.0."
  },
  {
    "objectID": "curriculo.html#experiência-profissional",
    "href": "curriculo.html#experiência-profissional",
    "title": "Mutual Blog",
    "section": "",
    "text": "Axios Investimentos\n\n\nFunção\nLocal\nInicio\nFim\nDescrição\n\n\n\n\nOperador de Renda variável\nNatal, RN\njul/19\nmar/20\nAtuava no processo de automatização da mesa de renda variável, o qual envolvia a geração de ordens para diversos clientes com o objetivo de aderir às trocas de ativos sugeridos nas carteiras recomendadas pelas researchs que utilizamos. Esse processo incluiu a criação de um dashboard que gerenciava todas as etapas, desde o cadastro dos clientes e carteiras até a geração de baskets de ordens, as quais eram utilizadas para alimentar as plataformas de negociação.\n\n\n\n\n\n\n\n\n7RD Investimentos\n\n\nFunção\nLocal\nInicio\nFim\nDescrição\n\n\n\n\nBI\nNatal, RN\njul/20\nabr/22\nDesenvolvi ferramentas de BI na empresa, proporcionando dados para gestores e assessores por meio de dashboards. Criei um sistema eficiente para calcular comissões e desenvolvi robôs automatizando rotinas administrativas.\n\n\nOperador de Renda variável\nNatal, RN\nnov/21\njan/23\nAtuava como operador na mesa de operações, auxiliando os clientes no processo de envio de ordens de renda variável. Além disso, desenvolvi um dashboard de controle de produção da mesa, que indicava diariamente a produção em nível de operador, assessor e cliente.\n\n\nHead Operacional\nNatal, RN\nmar/22\nAtual\nAtuo como líder operacional, gerenciando os processos de análise de dados do escritório e as operações operacionais. Aqui, lidero o time de backoffice, composto principalmente por assessores em treinamento, e resolvo problemas mais complexos quando necessário. Além dos processos operacionais habituais, também estou envolvido na automação das rotinas financeiras, como o cálculo de impostos e comissões."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#textosposts",
    "href": "index.html#textosposts",
    "title": "Mutual Blog",
    "section": "Textos/Posts",
    "text": "Textos/Posts"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Ordenar por\n       Pré-selecionado\n         \n          Título\n        \n         \n          Data - Mais velho\n        \n         \n          Data - O mais novo\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDesempenho dos Fundos de Renda Fixa\n\n\n\n1 de mar. de 2025\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "posts/desempenho_fundos_rf/index.html",
    "href": "posts/desempenho_fundos_rf/index.html",
    "title": "Desempenho dos Fundos de Renda Fixa",
    "section": "",
    "text": "Desde que concluí minha tese sobre fundos de investimento, no início de 2014, venho considerando publicar posts regulares sobre o tema. O objetivo é construir uma base de conhecimento e compartilhar insights relevantes. Embora existam páginas que comparam fundos, ainda são raras as discussões que abordam a indústria de forma mais abrangente, analisando aspectos como captação, rentabilidade, risco e diversas outras métricas. Neste espaço, a proposta é analisar os dados de forma agregada, sem focar em fundos individuais.\nNeste primeiro post, abordaremos questões mais técnicas, como o download dos dados, tratamento, integração de bases e outros processos necessários para nossas análises. Começaremos analisando as três principais classes de fundos: renda fixa, multimercados e ações. Esta será a primeira de uma série de três publicações, cada uma dedicada a uma das classes mencionadas.\nVamos cobrir todas as etapas do processo, como coleta dos dados, limpeza, manipulação e armazenamento. Além do processamento dos dados, vamos criar nossas análises sobre essas informações, a fim de entender o comportamento da indústria de fundos.\n\nFonte de Dados\nVamos utilizar como base de dados primária os dados presentes na CVM, de onde pegaremos todos os dados referentes aos dados de cotas, dados cadastrais e o que é chamado de extrato de informações. Além disso, vamos utilizar dados fornecidos pela ANBIMA. Vamos utilizar as classificações dos fundos presentes nos dados cadastrais que a instituição utiliza. A escolha de utilização da classificação ANBIMA, está no fato de que durante o processo de tratamento de dados em minha tese de doutorado, percebi algumas inconsistências na base da CVM.\n\n\nAquisição de dados\nOs primeiros dados que iremos baixar são aqueles referentes às cotas, ao PL e à captação dos fundos de investimento disponíveis no portal de dados abertos da CVM. O nome da base é: Informe Diário.\nA CVM nos alerta que esses dados podem ser reapresentados em até M-11, além de dividi-los em dois repositórios: um contendo dados históricos desde 2000 até 2020, e outro com dados de 2021 até os dias atuais. Vamos baixar ambos. Faremos o download de arquivos compactados, que serão descompactados e carregados em nossa base de dados. Após a descompactação, uniremos todos os arquivos em uma única base.\nPara lidar com o grande volume de dados, vamos paralelizar nossa execução usando o pacote furrr. Além disso, utilizaremos os pacotes dplyr e tidyr para realizar a manipulação dos dados em memória, enquanto o pins nos servirá como repositório para armazená-los. Para nosso banco de dados, adotaremos o DuckDB, o que nos permitirá trabalhar de forma eficiente com grandes tabelas em disco, e contaremos ainda com o pacote dbplyr para manipular os dados diretamente no banco.\nPrimeira coisa que vamos fazer é carrregar os pacotes necessários e conectar ao nosso banco. Lembro que se você for fazer isos em sua máquina o caminho que está sendo utilizando no parâmetro dbdir tem que ser relativo a sua própria pasta.\n\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(tidyr)\nlibrary(furrr)\n\n# Conectando a base de dados\n\ncon &lt;- DBI::dbConnect(duckdb::duckdb(), \n                 dbdir = \"fundos_db.duckdb\",\n                 read_only = FALSE)\n\nPara obter o conjunto completo de dados, vamos acessar os endereços HIST e DADOS para baixar todos os arquivos disponíveis. Para otimizar essa etapa, utilizaremos as funções de paralelização do pacote furrr, configurando quatro clusters para realizar as requisições simultaneamente e, assim, agilizar o processo de download.\n\n# Criando a sequência de datas para criar as urls de download\n\nanos_download &lt;- seq(2000,2020,1)\nmeses_download &lt;- format(seq(as.Date(\"2021-01-01\"),\n                             as.Date(\"2025-01-01\"),\n                             by = \"month\"),\n                         \"%Y%m\")\n\n# Definindo o número de workers\nfuture::plan(future::multisession, \n             workers = 6)\n\n# Fazendo donwload dos arquivos de 2000 a 2020\n\nfurrr::future_walk(anos_download, function(ano){\n  options(timeout = 600)\n  url &lt;- paste0(\"https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/HIST/inf_diario_fi_\",\n                ano,\n                \".zip\")\n  destfile &lt;- paste0(\"./informe_diario/\",ano,\".zip\")\n  utils::download.file(url,destfile,mode = \"wb\")\n},.progress = T)\n\nzipfiles &lt;- list.files(\"./informe_diario/\",full.names = T)\n\nfurrr::future_walk(zipfiles,function(path){\n  unzip(path,exdir = \"./informe_diario/\")\n})\n             \n# Fazendo donwload dos arquivos de 2021 a janeiro de 2025 \n\nfurrr::future_walk(meses_download, function(mes){\n  options(timeout = 600)\n  url &lt;- paste0(\"https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_\",\n                mes,\n                \".zip\")\n  destfile &lt;- paste0(\"./informe_diario/\",mes,\".zip\")\n  download.file(url,destfile)\n})\n\n# Dezipando arquivos \n\nzipfiles &lt;- list.files(\"./informe_diario/\",full.names = T)\n\nfurrr::future_walk(zipfiles,function(path){\n  unzip(path,exdir = \"./informe_diario/\")\n})\n\n# Deletando arquivos baixados\n\ninvisible(file.remove(list.files(\"./posts/desempenho_fundos_rf/informe_diario/\",\n                                 full.names = T)))\n\n# Excluindo os workers da paralelização\nfuture::plan(future::sequential)\n\nDepois de baixar todos os arquivos, precisamos ler cada um dos CSVs e unificá-los em um único objeto. Essa etapa pode ser bastante exigente em termos de recursos e, por isso, pode não rodar tão facilmente em alguns computadores. No meu caso, como a minha máquina tem 32 GB de memória, consegui processar tudo sem maiores problemas.\n\narquivos_csv &lt;- list.files(\"./informe_diario/\",full.names = T,pattern = \".csv\")\n\nfuture::plan(future::multisession, workers = 6)\nfurrr::future_map(arquivos_csv, function(arquivo){\n  readr::read_delim(arquivo, \n    delim = \";\", escape_double = FALSE, trim_ws = TRUE)\n},.progress = T) -&gt; informes_diarios\n\nfuture::plan(future::sequential)\n\ninformes_diarios %&gt;% \n  bind_rows() -&gt; informes_diarios\n\nDeltando o objeto com todos os CSVs lidos.\n\nDBI::dbWriteTable(con,\"informes_diarios\",informes_diarios)\nrm(informes_diarios)\n\n\n\nETL dos Dados\nPrimeiro passo para começarmos o ETL de nossos dados consiste em conetarmos com nossa base de dados. Vamos utilizar a função do dplyr tbl. Isso nos permite lidar com a tabela que está no nosso banco do DuckDB como se fosse um DF dentro de nosso projeto. Os arquivos que baixamos possuem todos os tipos de fundos. Aqui vamos nos concentrar nos fundos de investimentos tradicionais, os conhecido por fazerem parte da instrução CVM 175. Para fazer isso vamos filtrar nossa base somente com estes fundos, onde vamos filtrar somente os fundos com a designação FI na base.\n\ninformes_diarios_fi &lt;- tbl(con,\"informes_diarios\") %&gt;% \n  filter(TP_FUNDO == \"FI\")\n\nVamos começar integrando à base de dados fornecida pela ANBIMA. Primeiro, leremos o arquivo e o salvaremos em nossa base do DuckDB. Em seguida, faremos a união das duas fontes de dados, o que nos permitirá filtrar apenas as classes de fundos que desejamos para este primeiro momento, focando nos fundos de renda fixa.\n\nfundos_175_anbima &lt;- readxl::read_excel(\"fundos-175-caracteristicas-publico-22-02-2025-12-01-14.xlsx\")\nDBI::dbWriteTable(con,\"fundos_175_anbima\",fundos_175_anbima)\nrm(fundos_175_anbima)\n\nTexto revisado e sugestões de melhoria:\nAqui, como os dados filtrados resultaram em uma quantidade menor, o tamanho da base nos permite trabalhar tudo dentro do R sem problemas de desempenho. O DuckDB é fundamental para filtrar e unir as bases, pois o tempo de execução é bastante reduzido devido à sua maior eficiência ao lidar com grandes volumes de dados. Apenas a base de FI, por exemplo, possui cerca de 20 milhões de linhas, o que deixaria a análise diretamente no R muito lenta. Ao reduzir o tamanho da base, passamos a poder trabalhar tudo dentro do próprio R, sem precisar utilizar o DuckDB. É isso que fazemos a seguir.\n\ninformes_diarios_fi %&gt;% \n  mutate(CNPJ_FUNDO = sql(\"regexp_replace(CNPJ_FUNDO, '[^0-9]', '', 'g')\")) %&gt;% \n  left_join(\n    tbl(con,\"fundos_175_anbima\") %&gt;% \n      select(`Código ANBIMA`,\n             Estrutura,\n             `CNPJ do Fundo`,\n             `Nome Comercial`,\n             Status,\n             `Categoria ANBIMA`,\n             `Tipo ANBIMA`,\n             `Composição do Fundo`,\n             `Aberto Estatutariamente`,\n              `Tributação Alvo`,\n             `Primeiro Aporte`,\n             `Tipo de Investidor`,\n             `Característica do Investidor`,\n             `Cota de Abertura`,\n             `Aplicação Inicial Mínima`,\n             `Prazo Pagamento Resgate em dias`),\n    by = c(\"CNPJ_FUNDO\" = \"CNPJ do Fundo\")\n  ) %&gt;% \n  filter(`Categoria ANBIMA` == \"Renda Fixa\") %&gt;% \n  collect()-&gt; fundos_rf\n\nDepois de realizar esses processamentos, vamos conferir a quantidade de fundos com os quais trabalharemos.\n\nfundos_rf %&gt;% \n  distinct(CNPJ_FUNDO,`Categoria ANBIMA`,`Tipo ANBIMA`) %&gt;% \n  count(`Categoria ANBIMA`,`Tipo ANBIMA`) %&gt;% \n  arrange(-n) %&gt;% \n  rename(Quantidade = n) %&gt;% \n  reactable::reactable(\n    pagination = FALSE,  # Remove a paginação da tabela\n    columns = list(\n      # Garante mais espaço para os nomes longos\n      `Tipo ANBIMA` = reactable::colDef(width = 400), \n      Quantidade = reactable::colDef(\n        align = \"right\",\n        # Formata números com separador de milhar\n        format = reactable::colFormat(separators = TRUE)\n      )\n    ),\n    # Garante que todas as colunas tenham um tamanho mínimo\n    defaultColDef = reactable::colDef(minWidth = 100)  \n  )"
  },
  {
    "objectID": "posts/desempenho_fundos_rf/index.html#fonte-de-dados",
    "href": "posts/desempenho_fundos_rf/index.html#fonte-de-dados",
    "title": "Desempenho dos Fundos de Renda Fixa",
    "section": "Fonte de Dados",
    "text": "Fonte de Dados"
  }
]