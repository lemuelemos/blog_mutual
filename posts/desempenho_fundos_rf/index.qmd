---
title: "Desempenho dos Fundos de Renda Fixa"
author: "Lemuel Lemos"
date: "2025-03-01"
categories: [fundos de investimentos, desempenho, renda fixa]
image: "image.jpg"
execute:
  warning: false
  message: false
---

Desde que concluí minha tese sobre fundos de investimento, no início de 2014, venho considerando publicar posts regulares sobre o tema. O objetivo é construir uma base de conhecimento e compartilhar insights relevantes. Embora existam páginas que comparam fundos, ainda são raras as discussões que abordam a indústria de forma mais abrangente, analisando aspectos como captação, rentabilidade, risco e diversas outras métricas. Neste espaço, a proposta é analisar os dados de forma agregada, sem focar em fundos individuais.

Neste primeiro post, abordaremos questões mais técnicas, como o download dos dados, tratamento, integração de bases e outros processos necessários para nossas análises. Começaremos analisando as três principais classes de fundos: renda fixa, multimercados e ações. Esta será a primeira de uma série de três publicações, cada uma dedicada a uma das classes mencionadas.

Vamos cobrir todas as etapas do processo, como coleta dos dados, limpeza, manipulação e armazenamento. Além do processamento dos dados, vamos criar nossas análises sobre essas informações, a fim de entender o comportamento da indústria de fundos.

### Fonte de Dados

Vamos utilizar como base de dados primária os dados presentes na <a href="https://dados.cvm.gov.br/group/fundos-de-investimento" target="_blank">CVM</a>, de onde pegaremos todos os dados referentes aos dados de cotas, dados cadastrais e o que é chamado de extrato de informações. Além disso, vamos utilizar dados fornecidos pela ANBIMA. Vamos utilizar as classificações dos fundos presentes nos dados cadastrais que a instituição utiliza. A escolha de utilização da classificação ANBIMA, está no fato de que durante o processo de tratamento de dados em minha tese de doutorado, percebi algumas inconsistências na base da CVM. 

### Aquisição de dados

Os primeiros dados que iremos baixar são aqueles referentes às cotas, ao PL e à captação dos fundos de investimento disponíveis no portal de dados abertos da <a href="https://dados.cvm.gov.br/group/fundos-de-investimento" target="_blank">CVM</a>. O nome da base é: <a href="https://dados.cvm.gov.br/dataset/fi-doc-inf_diario" target="_blank">Informe Diário</a>.

A CVM nos alerta que esses dados podem ser reapresentados em até M-11, além de dividi-los em dois repositórios: um contendo dados históricos desde 2000 até 2020, e outro com dados de 2021 até os dias atuais. Vamos baixar ambos. Faremos o download de arquivos compactados, que serão descompactados e carregados em nossa base de dados. Após a descompactação, uniremos todos os arquivos em uma única base.

Para lidar com o grande volume de dados, vamos paralelizar nossa execução usando o pacote `furrr`. Além disso, utilizaremos os pacotes `dplyr` e `tidyr` para realizar a manipulação dos dados em memória, enquanto o `pins` nos servirá como repositório para armazená-los. Para nosso banco de dados, adotaremos o **DuckDB**, o que nos permitirá trabalhar de forma eficiente com grandes tabelas em disco, e contaremos ainda com o pacote `dbplyr` para manipular os dados diretamente no banco.

Primeira coisa que vamos fazer é carrregar os pacotes necessários e conectar ao nosso banco. Lembro que se você for fazer isos em sua máquina o caminho que está sendo utilizando no parâmetro `dbdir` tem que ser relativo a sua própria pasta. 

```{r carregamento de pacotes,}
library(dplyr)
library(dbplyr)
library(tidyr)
library(furrr)

# Conectando a base de dados

con <- DBI::dbConnect(duckdb::duckdb(), 
                 dbdir = "fundos_db.duckdb",
                 read_only = FALSE)

```

Para obter o conjunto completo de dados, vamos acessar os endereços [HIST](https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/HIST/) e [DADOS](https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/) para baixar todos os arquivos disponíveis. Para otimizar essa etapa, utilizaremos as funções de paralelização do pacote `furrr`, configurando quatro clusters para realizar as requisições simultaneamente e, assim, agilizar o processo de download.

```{r download de arquivos, cache=T, eval = FALSE}
# Criando a sequência de datas para criar as urls de download

anos_download <- seq(2000,2020,1)
meses_download <- format(seq(as.Date("2021-01-01"),
                             as.Date("2025-01-01"),
                             by = "month"),
                         "%Y%m")

# Definindo o número de workers
future::plan(future::multisession, 
             workers = 6)

# Fazendo donwload dos arquivos de 2000 a 2020

furrr::future_walk(anos_download, function(ano){
  options(timeout = 600)
  url <- paste0("https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/HIST/inf_diario_fi_",
                ano,
                ".zip")
  destfile <- paste0("./informe_diario/",ano,".zip")
  utils::download.file(url,destfile,mode = "wb")
},.progress = T)

zipfiles <- list.files("./informe_diario/",full.names = T)

furrr::future_walk(zipfiles,function(path){
  unzip(path,exdir = "./informe_diario/")
})
             
# Fazendo donwload dos arquivos de 2021 a janeiro de 2025 

furrr::future_walk(meses_download, function(mes){
  options(timeout = 600)
  url <- paste0("https://dados.cvm.gov.br/dados/FI/DOC/INF_DIARIO/DADOS/inf_diario_fi_",
                mes,
                ".zip")
  destfile <- paste0("./informe_diario/",mes,".zip")
  download.file(url,destfile)
})

# Dezipando arquivos 

zipfiles <- list.files("./informe_diario/",full.names = T)

furrr::future_walk(zipfiles,function(path){
  unzip(path,exdir = "./informe_diario/")
})

# Deletando arquivos baixados

invisible(file.remove(list.files("./posts/desempenho_fundos_rf/informe_diario/",
                                 full.names = T)))

# Excluindo os workers da paralelização
future::plan(future::sequential)
```

Depois de baixar todos os arquivos, precisamos ler cada um dos CSVs e unificá-los em um único objeto. Essa etapa pode ser bastante exigente em termos de recursos e, por isso, pode não rodar tão facilmente em alguns computadores. No meu caso, como a minha máquina tem 32 GB de memória, consegui processar tudo sem maiores problemas.


```{r leitura dos csvs,cache=T, eval = FALSE}
arquivos_csv <- list.files("./informe_diario/",full.names = T,pattern = ".csv")

future::plan(future::multisession, workers = 6)
furrr::future_map(arquivos_csv, function(arquivo){
  readr::read_delim(arquivo, 
    delim = ";", escape_double = FALSE, trim_ws = TRUE)
},.progress = T) -> informes_diarios

future::plan(future::sequential)

informes_diarios %>% 
  bind_rows() -> informes_diarios

```

Deltando o objeto com todos os CSVs lidos.

```{r linkando base, eval = FALSE}
DBI::dbWriteTable(con,"informes_diarios",informes_diarios)
rm(informes_diarios)
```
### ETL dos Dados

Primeiro passo para começarmos o ETL de nossos dados consiste em conetarmos com nossa base de dados. Vamos utilizar a função do `dplyr` `tbl`. Isso nos permite lidar com a tabela que está no nosso banco do DuckDB como se fosse um DF dentro de nosso projeto. Os arquivos que baixamos possuem todos os tipos de fundos. Aqui vamos nos concentrar nos fundos de investimentos tradicionais, os conhecido por fazerem parte da instrução CVM 175. Para fazer isso vamos filtrar nossa base somente com estes fundos, onde vamos filtrar somente os fundos com a designação **FI** na base. 

```{r carregamento de base}
informes_diarios_fi <- tbl(con,"informes_diarios") %>% 
  filter(TP_FUNDO == "FI")
```

Vamos começar integrando à base de dados fornecida pela ANBIMA. Primeiro, leremos o arquivo e o salvaremos em nossa base do DuckDB. Em seguida, faremos a união das duas fontes de dados, o que nos permitirá filtrar apenas as classes de fundos que desejamos para este primeiro momento, focando nos fundos de renda fixa.

```{r carregando dados anbima, eval=F}
fundos_175_anbima <- readxl::read_excel("fundos-175-caracteristicas-publico-22-02-2025-12-01-14.xlsx")
DBI::dbWriteTable(con,"fundos_175_anbima",fundos_175_anbima)
rm(fundos_175_anbima)
```


```{r}
informes_diarios_fi %>% 
  mutate(CNPJ_FUNDO = sql("regexp_replace(CNPJ_FUNDO, '[^0-9]', '', 'g')")) %>% 
  left_join(
    tbl(con,"fundos_175_anbima") %>% 
      select(`Código ANBIMA`,
             Estrutura,
             `CNPJ do Fundo`,
             `Nome Comercial`,
             Status,
             `Categoria ANBIMA`,
             `Tipo ANBIMA`,
             `Composição do Fundo`,
             `Aberto Estatutariamente`,
              `Tributação Alvo`,
             `Primeiro Aporte`,
             `Tipo de Investidor`,
             `Característica do Investidor`,
             `Cota de Abertura`,
             `Aplicação Inicial Mínima`,
             `Prazo Pagamento Resgate em dias`),
    by = c("CNPJ_FUNDO" = "CNPJ do Fundo")
  ) %>% 
  filter(`Categoria ANBIMA` == "Renda Fixa") %>% 
  collect()-> fundos_rf
```

Depois de realizar esses processamentos, vamos conferir a quantidade de fundos com os quais trabalharemos.


```{r tabela de fundos por classe e tipo}
fundos_rf %>% 
  distinct(CNPJ_FUNDO,`Categoria ANBIMA`,`Tipo ANBIMA`) %>% 
  count(`Categoria ANBIMA`,`Tipo ANBIMA`) %>% 
  arrange(-n) %>% 
  rename(Quantidade = n) %>% 
  reactable::reactable(
    pagination = FALSE,  # Remove a paginação da tabela
    columns = list(
      `Tipo ANBIMA` = reactable::colDef(width = 400),  # Garante mais espaço para os nomes longos
      Quantidade = reactable::colDef(
        align = "right",
        format = reactable::colFormat(separators = TRUE)  # Formata números com separador de milhar
      )
    ),
    defaultColDef = reactable::colDef(minWidth = 100)  # Garante que todas as colunas tenham um tamanho mínimo
  )
```
